
1
00:00:00,000 --> 00:00:06,003
Hello. Welcome to the course on computer
vision. My name is Jitendra Malik, and I'm

2
00:00:06,003 --> 00:00:12,003
in the Department of EECS at UC Berkeley.
So, let's begin with the beginning.

3
00:00:12,003 --> 00:00:18,007
A camera creates an image of the external
world. And computer vision is the science

4
00:00:18,007 --> 00:00:23,001
and engineering discipline which is
concerned with making inferences about

5
00:00:23,001 --> 00:00:27,008
this external world, given one or more
of its images. I mean the part about

6
00:00:27,008 --> 00:00:32,007
science and engineering. Science comes in
because there are fundamental principles

7
00:00:32,007 --> 00:00:37,003
associated with the physics of image
formation, And statistical regularities in

8
00:00:37,003 --> 00:00:41,004
the external world which makes this
enterprise at all possible.

9
00:00:41,004 --> 00:00:46,003
It's engineering because there are many, many
practical applications of what we'll study

10
00:00:46,003 --> 00:00:52,002
in this course. And we'll get to see some
of them. Let's begin with what's in an

11
00:00:52,002 --> 00:00:57,009
image. So here's this image, and you might
look at it and say there's a tiger and

12
00:00:57,009 --> 00:01:03,007
grass. But what does a computer program
see? For the computer program, it's just a

13
00:01:03,007 --> 00:01:08,006
bunch of pixels. And they have some
values, some RGB values, and here's

14
00:01:08,006 --> 00:01:13,009
another window and there are some other
RGB values. The only way we get any

15
00:01:13,009 --> 00:01:19,003
meaning out of this is by the activity of
the brain. It's the humans that perceive

16
00:01:19,003 --> 00:01:24,005
structure in it, otherwise you just have
an area of values. The goal in computer

17
00:01:24,005 --> 00:01:29,006
vision is to give computers the same
capability. So in a grand sense, our goal

18
00:01:29,006 --> 00:01:34,005
is to go from pixels to perception. So
let's being with this image, What can we

19
00:01:34,005 --> 00:01:39,005
do with this image? One of the things we
can do is to divide up the image into

20
00:01:39,005 --> 00:01:44,006
various regions. So here I marked out the
region, colors pointing to the tiger. I

21
00:01:44,006 --> 00:01:49,008
can describe that region. I can say it's
orange. I can say that it has some texture

22
00:01:49,008 --> 00:01:55,002
stripes in it. We can go further. We can
attach some semantic labels. We can say

23
00:01:55,002 --> 00:02:00,007
there's water, grass, tigers, sun and so
on. We can continue this process further,

24
00:02:00,009 --> 00:02:06,006
not only can we label the tiger, but we
can label parts of the tiger. Such as its

25
00:02:06,006 --> 00:02:11,009
back, head, eye, mouth and so on. So this
is what we humans can perceive in an

26
00:02:11,009 --> 00:02:18,007
image, and the goal of computer vision is
to achieve something similar to this. Once

27
00:02:18,007 --> 00:02:24,004
a upon a time we had a very feet forward
processing view of vision and there were

28
00:02:24,004 --> 00:02:30,000
various theories and models constructed in
which we started out by analyzing local

29
00:02:30,000 --> 00:02:35,003
neighborhoods of an image. We moved on
from that to analyzing the surfaces and

30
00:02:35,003 --> 00:02:40,001
contours of the object. So we are, I'm
trying to show here the surfaces

31
00:02:40,001 --> 00:02:45,001
corresponding to the tiger. The grass,
water, and so on. And then, finally, we

32
00:02:45,001 --> 00:02:50,001
went to high level inferences about
scenes, and the objects and the scenes.

33
00:02:50,001 --> 00:02:55,000
Each of these stages of processing had
natural techniques that could be

34
00:02:55,000 --> 00:03:00,002
associated with them. For example, if
you're analyzing neighborhoods of images,

35
00:03:00,002 --> 00:03:05,005
then image processing is the canonical
technique to be used, Linear filtering,

36
00:03:05,005 --> 00:03:10,004
convolution, and so on. When we try to
analyze the image of the levels of

37
00:03:10,004 --> 00:03:15,007
contours and surfaces, that's what is
called mid level vision. And here is where

38
00:03:15,007 --> 00:03:20,005
we have grouping operations, and
inferences about figure ground, and the

39
00:03:20,005 --> 00:03:26,004
surface attributes of the scene. And
finally we get to high level vision, where

40
00:03:26,004 --> 00:03:32,001
recognition is the main, goal. And, this
is where one draws on statistical

41
00:03:32,001 --> 00:03:38,006
techniques such as, pattern recognition,
classification, etc. So this feet forward

42
00:03:38,006 --> 00:03:43,008
view of vision is regarded as rather
simplistic these days. The way we think

43
00:03:43,008 --> 00:03:49,006
about it now is that we think that vision
has three interconnected processes. One of

44
00:03:49,006 --> 00:03:54,008
these is recognition of objects and
activities. So, giving objects names like,

45
00:03:54,008 --> 00:04:00,003
tiger, grass, and so on. Another process
is the one of reorganization, or sometimes

46
00:04:00,003 --> 00:04:05,007
it's called grouping and segmentation.
This involves inferences such as this set

47
00:04:05,007 --> 00:04:11,009
of pixels belong together to
one object. And then, we have

48
00:04:11,009 --> 00:04:17,003
reconstructing 3D structure. By that, we
mean, recovering depths of objects in the

49
00:04:17,003 --> 00:04:22,008
scene, how far away various objects are
with respect to the camera. So that's the

50
00:04:22,008 --> 00:04:27,004
third process. And all these processes
interrelate. So, the results of

51
00:04:27,004 --> 00:04:32,008
recognition can help with reconstruction,
the results of reconstruction can help

52
00:04:32,008 --> 00:04:39,006
with recogniti on, and so on and so forth.
So let's start talking about the state of

53
00:04:39,006 --> 00:04:44,002
the art of in recognition. One of the
earliest problems that was tackled in

54
00:04:44,002 --> 00:04:49,001
computer vision was that of 110 digit
recognition. And there are some standard

55
00:04:49,001 --> 00:04:54,000
data sets, one called MNEST, one called
USBS, and you can search for these on the

56
00:04:54,000 --> 00:04:59,002
Web. And we are by now pretty good at this
problem. So, one of the pioneers in this

57
00:04:59,002 --> 00:05:04,002
area is Gam Lakun, and he has developed
radius theorem and network architectures,

58
00:05:04,002 --> 00:05:10,002
For this problem. And, he has shown that
these architectures can give Error rates

59
00:05:10,002 --> 00:05:15,005
as low as on the order of half a percent
on these data sets. And the resulting

60
00:05:15,005 --> 00:05:21,001
systems are quite faster, and these can be
used in practice, and are being used in

61
00:05:21,001 --> 00:05:26,004
practice for check reading and so on.
There are a number of different methods

62
00:05:26,004 --> 00:05:32,001
out there which manage to achieve this.
I'd mentioned, like to mention randomized

63
00:05:32,001 --> 00:05:37,006
decision trees, because of [inaudible]
what is used in, some of the, the work on

64
00:05:37,006 --> 00:05:43,006
figuring out. Body parts in the algorithm
used in the connect system for example,

65
00:05:43,006 --> 00:05:49,004
Under the rubric of random forests. So
moving right along, there are other

66
00:05:49,004 --> 00:05:55,006
problems we can do quite well at. For
example, you've all encountered captures,

67
00:05:55,006 --> 00:06:02,000
these distorted text, that you have to
figure out what the text is to get a free

68
00:06:02,000 --> 00:06:08,001
email account, or get access to some
website, and so. Earlier line of work that

69
00:06:08,001 --> 00:06:13,004
we did some time ago that showed that
these Can in fact be recognized by

70
00:06:13,004 --> 00:06:18,002
computer programs. So maybe standard OCR
programs can't do this job, but the most

71
00:06:18,002 --> 00:06:23,002
advanced computer vision techniques can do
a perfectly good job at identifying the

72
00:06:23,002 --> 00:06:28,000
words even though they are in messed up
background or distorted in some way. So,

73
00:06:28,000 --> 00:06:33,000
let's talk a bit about general object
category recognition. Humans can recognize

74
00:06:33,000 --> 00:06:38,001
that all of these are faces even though
they look quite different from each other.

75
00:06:38,001 --> 00:06:43,000
Or that all of these are flowers even the,
though they look different from each

76
00:06:43,000 --> 00:06:48,006
other. And this is one of the fundamental
capabilities that we wish to give computer

77
00:06:48,006 --> 00:06:54,000
region systems. So one of the classic data
sets in this Was collected at Cal Tech,

78
00:06:54,003 --> 00:07:00,007
and it's called the Cal Tech 101 data set.
And this was collected by Fifi and Perona

79
00:07:00,007 --> 00:07:06,007
back in 2004. So what they did was to
collect examples from the web of images of

80
00:07:06,007 --> 00:07:12,008
101 different categories, and these could
include pianos, dogs, cats, faces, and so

81
00:07:12,008 --> 00:07:18,008
forth. And for each of these categories
they collected something like 30 to 300

82
00:07:18,008 --> 00:07:24,007
images. The images themselves were rather
simple. They had typically just one object

83
00:07:24,007 --> 00:07:30,001
in the centre of the image. But the
question was, would computer programs be

84
00:07:30,001 --> 00:07:37,008
able to find which of these and then what
different categories that object was. And,

85
00:07:38,005 --> 00:07:43,006
And this was a challenge now thrown open
to the entire computer vision community.

86
00:07:43,006 --> 00:07:48,008
And what I'm showing you here are some
curves which are performance curves where

87
00:07:48,008 --> 00:07:54,001
on the Y axis we're, there is some measure
of performance, which is, What percentage

88
00:07:54,001 --> 00:07:59,004
of the object were classified correctly?
And on the X axis here are examples. How

89
00:07:59,004 --> 00:08:04,005
many examples were used? So, for now you
can focus on just one vertical column

90
00:08:04,005 --> 00:08:09,004
corresponding to fifteen training
examples. So, what's interesting is that

91
00:08:09,004 --> 00:08:13,007
when the data-set first came out,
performance was at about sixteen percent

92
00:08:13,007 --> 00:08:18,004
correct. In a sense that's terrible. That
means that 84 percent of the time the

93
00:08:18,004 --> 00:08:23,005
program was wrong. But a positive way to
think about it is that if we were just

94
00:08:23,005 --> 00:08:27,007
performing a chance, performance would be
just one Percent. So sixteen percent was

95
00:08:27,007 --> 00:08:32,008
significantly better than chance. What's
very interesting is that in three years,

96
00:08:32,008 --> 00:08:37,008
due to the efforts of many different
groups, performance went up from sixteen

97
00:08:37,008 --> 00:08:41,002
percent to 65 percent correct. Which is a
factor of four improvement and

98
00:08:41,002 --> 00:08:45,008
performance. This is fantastic, this an
indication of a field that is finally

99
00:08:45,008 --> 00:08:50,005
getting its act together. And, today we
don't work that much on its data set

100
00:08:50,005 --> 00:08:55,003
anymore, because we prefer data sets where
there will be multiple objects in the

101
00:08:55,003 --> 00:08:59,009
scene. But, this was a good indicator that
the field was making progress and

102
00:08:59,009 --> 00:09:05,000
recognition. So I will give you an example
of what kinds of data sets are popular

103
00:09:05,000 --> 00:09:10,000
these days, The so called Pascal visual
object challenge. And this has been the

104
00:09:10,000 --> 00:09:14,009
dominant data set in the field for the
last five years. And this has much more

105
00:09:14,009 --> 00:09:19,008
complex scenes. As you can see in these
images, you typically have multiple

106
00:09:19,008 --> 00:09:25,002
objects. In an images, for example this
image you see a TV monitor, and a person,

107
00:09:25,002 --> 00:09:30,005
and a keyboard, and so on an so forth. In
this image you have a sheep and the

108
00:09:30,005 --> 00:09:36,002
presence of Grass and the goal of this
challenge was to test whether we could do

109
00:09:36,002 --> 00:09:41,003
recognition in these more difficult
circumstances. In these more difficult

110
00:09:41,003 --> 00:09:47,000
circumstances the goal was we should be
able to mark That A bounding box

111
00:09:47,000 --> 00:09:51,008
corresponding to the object. So somehow
you can mark the position of the sheep, or

112
00:09:51,008 --> 00:09:56,006
the position of the dog. And the way we
mark these is by drawing a bounding box.

113
00:09:56,006 --> 00:10:01,005
And when I say we, I really mean the
computer program. And the way we judge

114
00:10:01,005 --> 00:10:07,002
performance here is by, so called
precision recall curves. And you'll

115
00:10:07,002 --> 00:10:13,008
encounter this repeatedly in the, in the
course. So precision is measured here on

116
00:10:13,008 --> 00:10:21,001
the X, on the, on the Y axis. And, let me
use a. So precision here is on the Y axis.

117
00:10:21,001 --> 00:10:26,005
So precision means what percentage of the
objects that, computer program declared

118
00:10:26,005 --> 00:10:31,008
were bicycles were actually bicycles. So
ideally, you want precision to be one. And

119
00:10:31,008 --> 00:10:36,009
then, we also measure recall. Recall is,
how many of the bicycles in the data set

120
00:10:36,009 --> 00:10:41,009
did we manage to find? And of course you
would ideally want the recall to be one,

121
00:10:41,009 --> 00:10:46,007
and position to be one. In practice, we
are not so lucky. And so what happens is

122
00:10:46,007 --> 00:10:51,005
that there is some threshold parameter,
and if we have a more lax threshold, you

123
00:10:51,005 --> 00:10:56,002
get high recall. If you get more strict
threshold, you have high precision. And

124
00:10:56,002 --> 00:11:01,000
typically you get a curve, as shown here.
By varying the threshold, and the

125
00:11:01,000 --> 00:11:06,003
threshold here is being made more lax, and
as a result you increase recall and lower

126
00:11:06,003 --> 00:11:11,003
precision. So the ideal point, of course,
would be a precision recall of one, as I

127
00:11:11,003 --> 00:11:16,006
have marked here. And different algorithms
have different curves, and those are

128
00:11:16,006 --> 00:11:21,007
shown here. And you can get to these
curves by searching for Pascal VOC on

129
00:11:21,007 --> 00:11:27,002
the web. And the area under this curve
becomes a measure of the technique. And

130
00:11:27,002 --> 00:11:32,008
ideal curve would have an area under the
curve of one. But in practice, we get

131
00:11:32,008 --> 00:11:37,009
areas such as 55.3 percent, or 54.3%. So
in a certain sense, these numbers which

132
00:11:37,009 --> 00:11:43,002
are close to 50 percent tell us that we
are sort of halfway through solving this

133
00:11:43,002 --> 00:11:48,005
problem of detection of bicycles. Let's
move along to analysis of images of

134
00:11:48,005 --> 00:11:53,006
people. People are some of, one of the
most difficult categories to analyze.

135
00:11:53,006 --> 00:11:58,008
Just, if you look at this image, you will
see the variety in appearance, due to

136
00:11:58,008 --> 00:12:04,001
clothing, due to pose, due to occlusion,
and so on and so forth. And yet, people

137
00:12:04,001 --> 00:12:09,008
are a very important category because if
we can analyze images of people, there are

138
00:12:09,008 --> 00:12:15,003
a variety of applications that will become
accessible to us. So. I'll give you an

139
00:12:15,003 --> 00:12:21,000
example. Even from single images, just the
pose and appearance of a person can tell

140
00:12:21,000 --> 00:12:26,004
you whether a person is falling or running
or walking or riding a horse. And, in

141
00:12:26,004 --> 00:12:33,000
fact, computer programs are able to do
this. Let me look at, look at actions more

142
00:12:33,000 --> 00:12:38,002
generally. So there are a variety of
actions that we are interested in. Some of

143
00:12:38,002 --> 00:12:43,005
these actions correspond to movement and
posture change. So, these are examples,

144
00:12:43,005 --> 00:12:48,009
run walk, crawl. In fact you can think of
actions as English words, and objects as

145
00:12:48,009 --> 00:12:54,001
nouns. A large class of actions have to do
with object manipulation. So, these

146
00:12:54,001 --> 00:12:59,005
include the examples here. Pick an object,
carry an object, hold an object, lift an

147
00:12:59,005 --> 00:13:04,006
object, and so on. And we would like to
able to describe what's happening in an

148
00:13:04,006 --> 00:13:10,000
image by using these terms. So, of course,
actions include, conversational gesture. I

149
00:13:10,000 --> 00:13:14,007
mean, in this example, suppose I move my
hands about. That's conversational

150
00:13:14,007 --> 00:13:20,008
gesture. And of course, sign language.
Recognition is not limited to objects and

151
00:13:20,008 --> 00:13:25,009
actions. We might be, we are able to
recognize materials. So in this example,

152
00:13:25,009 --> 00:13:31,002
looking at this image, you might be able
to guess what the material. Is it felt,

153
00:13:31,002 --> 00:13:35,009
polyester, rough plaster, leather,
concrete, and so on. And in computer

154
00:13:35,009 --> 00:13:41,009
vision we seek to develop techniques for
recognizing materials. So let's return to

155
00:13:41,009 --> 00:13:48,006
the central problems of vision. I've spent
a lot of time talking about the different

156
00:13:48,008 --> 00:13:55,004
sub-aspects of the problem of recognition
of objects in activities. Lets turn now to

157
00:13:55,004 --> 00:14:02,000
reorganization. So the task of organizing
an image from pixels into objects. So here

158
00:14:02,000 --> 00:14:08,006
are some examples, segmentations from the
work of segmentation data set. What we did

159
00:14:08,006 --> 00:14:14,009
was we took images, such as shown here in
the left column. And for each of these

160
00:14:14,009 --> 00:14:21,005
images what we did was we asked various
human subjects to look at these images and

161
00:14:21,005 --> 00:14:27,003
to mark out the objects in them. And what
you see are different people's.

162
00:14:27,003 --> 00:14:33,001
Segmentations of these images. But what
you see clearly is that people are able to

163
00:14:33,001 --> 00:14:38,006
mark the objects. So if you look at this
example, they can clearly mark the two

164
00:14:38,006 --> 00:14:43,006
worlds. In some, some. Humans are more
detailed. They mark out all the object,

165
00:14:43,006 --> 00:14:48,008
and the parts of objects, and some will do
less. It's clear that we have this

166
00:14:48,008 --> 00:14:53,004
capability of marking out the set of
pixels which correspond to a single

167
00:14:53,004 --> 00:14:58,007
object. This is an ability that we wish to
have computer programs replicate. Here are

168
00:14:58,007 --> 00:15:03,004
more examples. So, this, I like this
example of the butterfly on a, on a tree.

169
00:15:03,004 --> 00:15:08,002
Because, examples of objects against the
natural background really challenge,

170
00:15:08,003 --> 00:15:13,007
segmentation systems. Because objects
evolve animals evolve camouflage so that

171
00:15:13,007 --> 00:15:18,000
they can be hidden against their
background. And that makes the task of

172
00:15:18,000 --> 00:15:23,002
detecting them against those backgrounds
particularly challenging. So you can see

173
00:15:23,002 --> 00:15:30,009
that in these examples. So how do we do
this? Of course we're gonna spend some

174
00:15:30,009 --> 00:15:37,004
time on this but there are a number of, so
let's look at, take a concrete example. So

175
00:15:37,004 --> 00:15:43,004
here is this, this, this wooden bed stand.
I, then, here is the vegan correspondent

176
00:15:43,004 --> 00:15:48,007
to the bedspread. And how do we find the
boundaries between these? We can find the

177
00:15:48,007 --> 00:15:53,006
boundaries with a number of low-level
cues, such as brightness in color, one is

178
00:15:53,006 --> 00:15:58,005
green, one is beige, the te xture. This is
a single image, if we had, if we had a

179
00:15:58,005 --> 00:16:03,006
moving camera then the nearer object would
move faster, and that would give you a

180
00:16:03,006 --> 00:16:08,002
cue. Binocular disparity if we had two
eyes. Looking at this scene and there,

181
00:16:08,002 --> 00:16:12,009
there, then again, we have acute to depth.
And finally of course we can use familiar

182
00:16:12,009 --> 00:16:17,001
configuration, we can segment out a face
from the background once we have

183
00:16:17,001 --> 00:16:22,002
recognized that those pixels correspond to
a face. So we'll have an opportunity to

184
00:16:22,002 --> 00:16:28,004
study these variety of techniques later in
the course. Okay, so now I've done to the

185
00:16:28,004 --> 00:16:35,000
third fundamental problem of vision which
is that of reconstructing 3D structure.

186
00:16:35,000 --> 00:16:40,003
How do we do this? It's very helpful to
think about reconstruction as the inverse

187
00:16:40,003 --> 00:16:45,001
problem of computer graphics. So computer
graphics is the forward problem. What

188
00:16:45,001 --> 00:16:50,009
graphics people are able to do is, given
that scene geometry. By that, I mean

189
00:16:50,009 --> 00:16:56,005
the layout of the different objects,
models of the surfaces [inaudible] on the

190
00:16:56,005 --> 00:17:01,007
scene, and so on. As well as the
reflectance properties, The color. That,

191
00:17:01,007 --> 00:17:07,000
the, in a more general case it's called a
bidirectional reflectance distribution

192
00:17:07,000 --> 00:17:12,004
function, and of course the position and
distribution of the light sources. Given

193
00:17:12,004 --> 00:17:17,005
this, we can simulate the physics. We
simulate the physics of light transport,

194
00:17:17,005 --> 00:17:22,007
and we produce an image. That's what
computer graphics does. Computer vision in

195
00:17:22,007 --> 00:17:27,006
a certain sense is the inverse problem. We
are given an image, and we want to

196
00:17:27,006 --> 00:17:32,007
deconstruct what is the scene that gave
rise to this image. So they can of course

197
00:17:32,007 --> 00:17:37,005
be multiple images, and, and if we are
lucky. But our goal is to recover scene

198
00:17:37,005 --> 00:17:42,007
geometry, the reflectances of the various
objects in the scene, as well as the scene

199
00:17:42,007 --> 00:17:50,002
illuminations. So, How do I do this.
Recovery of geometry has historical roots

200
00:17:50,002 --> 00:17:56,006
in photo geometry and the analysis of 3D
Q's in human vision. But it's worth noting

201
00:17:56,006 --> 00:18:01,005
that it can be done. Even with single
images if we have some knowledge of the

202
00:18:01,005 --> 00:18:06,007
object class. And I'll give you an example
of that. Multiple images make the problem

203
00:18:06,007 --> 00:18:10,009
easier. But it's not trivial, as co
rresponding points must be identified.

204
00:18:10,009 --> 00:18:15,006
Once you have found corresponding points
than we can draw on triangulation. Of

205
00:18:15,006 --> 00:18:20,006
course there are settings when you have an
active camera, such as for example in the

206
00:18:20,006 --> 00:18:25,002
Kinect, where you project a pattern on the
image and that ability to predict a

207
00:18:25,002 --> 00:18:30,004
pattern gives you, an easy way to solve
the correspondence problem. So let's take

208
00:18:30,004 --> 00:18:36,000
up some examples. So, here's an example
from human vision, where binoculars

209
00:18:36,000 --> 00:18:41,006
stereopsis can be used. We have two
different eyes. They give you slightly

210
00:18:41,006 --> 00:18:47,003
different images of the scene, and the
difference between those two images is

211
00:18:47,003 --> 00:18:54,008
related to the depth of objects in the
scene. So that's one thing we can do. But

212
00:18:54,008 --> 00:19:01,001
of course we could use multiple views. So
here's an example of, 3D models that were

213
00:19:01,001 --> 00:19:07,001
constructed of Berkeley campus. This is
from a project going back nearly fifteen

214
00:19:07,001 --> 00:19:13,000
years, led by Paul Debevec. And what
they did was that they took a number of

215
00:19:13,000 --> 00:19:18,004
examples, a number of images of the
leading buildings, such as this clock

216
00:19:18,004 --> 00:19:23,007
tower. And, surrounding buildings on,
campus. Some of these pictures were taken

217
00:19:23,007 --> 00:19:28,005
with cameras on a kite. And from these
multiple pictures 3D models could be

218
00:19:28,005 --> 00:19:33,007
constructed. And this is of course now
standard technology and it's embodied in a

219
00:19:33,007 --> 00:19:39,006
number of different commercial products.
But here's an interesting example. With,

220
00:19:39,008 --> 00:19:45,007
extra assumptions even one images suffices. So
here's a reconstruction of the 3D geometry

221
00:19:45,007 --> 00:19:51,005
of the Taj Mahal. The Taj Mahal happens to
be a very symmetric figure, and therefore,

222
00:19:51,005 --> 00:19:56,008
even in a single image, you get the
equivalent of multiple views. And one can,

223
00:19:56,008 --> 00:20:06,003
in fact, reconstruct the three dimensional
shape of this object. Because an example

224
00:20:06,003 --> 00:20:11,005
of. Where we do not know what is the
object, but somehow when we look at this,

225
00:20:11,005 --> 00:20:16,009
this tablet or whatever they have this
perception that there must be some groves

226
00:20:16,009 --> 00:20:22,003
and there must be some ridges and so on. I
clearly hear it all the information is

227
00:20:22,003 --> 00:20:27,006
being carried by the pattern of light and
dark. What's worth noting is that these

228
00:20:27,006 --> 00:20:32,006
two images that you see here are really.
Essent ially the same image, it's just

229
00:20:32,006 --> 00:20:38,000
that one is a, one has been rotated by 180
degrees. And that rotation by 180 degrees

230
00:20:38,000 --> 00:20:43,001
is enough to change your perception. In
one case, what you saw as ridges, now you

231
00:20:43,001 --> 00:20:47,009
see as grooves. And why is that?
Well, that has to do with

232
00:20:47,009 --> 00:20:52,005
assumptions that the visual system makes about the
position of light sources. So in the

233
00:20:52,005 --> 00:20:57,007
environment that we grew up in, the light
comes from the sun and the sun is above.

234
00:20:57,007 --> 00:21:04,002
So there is going to be a certain pattern
Of shading associated with the ridge, and

235
00:21:04,002 --> 00:21:11,006
essentially when you flip... rotate the image by 180
degrees that assumption causes us to

236
00:21:11,006 --> 00:21:15,007
assume that the... now instead of looking at the ridge,
you're looking at the groove.

237
00:21:15,007 --> 00:21:21,003
Of course we can make
inferences about spatial layout, not just

238
00:21:21,003 --> 00:21:26,003
the shape of a single object. And this is
a kind of scene which is very common in

239
00:21:26,003 --> 00:21:31,005
Renaissance paintings, which showed these
courtyards which were full of rectangular

240
00:21:31,005 --> 00:21:36,003
slabs. And we are making a number of
assumptions here. We assume that the slabs

241
00:21:36,003 --> 00:21:41,000
are of the same size and shape in the
scene. But they look different in the

242
00:21:41,000 --> 00:21:45,009
image because some are nearer to you and
some are farther to you, away from you.

243
00:21:46,001 --> 00:21:51,000
This enables us to guess how far away and
at what orientation these objects are.

244
00:21:51,000 --> 00:21:59,007
Again, the big picture is the following.
Even when we have a single image, we are

245
00:21:59,007 --> 00:22:05,008
able to categorize The objects in, in
terms of depth. We can say some object is

246
00:22:05,008 --> 00:22:10,009
in front of some other object. So in this
kind of an example, this simple line

247
00:22:10,009 --> 00:22:16,000
drawing example, we can say that R1
appears in front of R2 and R2 appears in

248
00:22:16,000 --> 00:22:21,001
front of R3. And this we can do even with
a single image. Of course if we have

249
00:22:21,001 --> 00:22:25,009
multiple images such as in the case of
stereopsis then we will use that

250
00:22:25,009 --> 00:22:31,002
information to recover what objects are,
are nearer to you and which objects are

251
00:22:31,002 --> 00:22:38,007
farther away. What is all this for? In a
certain sense we can ask this fundamental

252
00:22:38,007 --> 00:22:43,005
question. Why do we try to recover
properties of the external world? And it's

253
00:22:43,005 --> 00:22:48,002
helpful here to think about biology. In
biology, why did vision evolve? Vision

254
00:22:48,002 --> 00:22:53,000
evolved because it gave the organism
which had vision some gain, some benefit.

255
00:22:53,000 --> 00:22:58,000
It had to give it some benefit in terms of
being able to find food, or to avoid becoming food.

256
00:22:58,000 --> 00:23:03,006
So, vision always is associated
with that ability to act in the

257
00:23:03,006 --> 00:23:09,000
world. And here, two examples of actions;
One is locomotion. You can move about and

258
00:23:09,000 --> 00:23:14,003
in which vision enables you to work with
maps of the environment,

259
00:23:14,003 --> 00:23:19,006
based on your previous experience and when you are
walking about you can avoid obstacles.

260
00:23:19,006 --> 00:23:25,002
Vision also allows you to manipulate objects. You
can grasp objects. You can pick and

261
00:23:25,002 --> 00:23:30,000
place objects. You can use tools. So,
these are all. Examples of hand eye

262
00:23:30,000 --> 00:23:35,008
coordination where the hand together with
the eye is used to perform action in the

263
00:23:35,008 --> 00:23:41,002
world. This is very important because
just. From an evolutionary point of view,

264
00:23:41,002 --> 00:23:46,005
a visual system which did not help with
action would not be of much use and

265
00:23:46,005 --> 00:23:51,009
clearly would not have any evolutionary
advantage that would create favorable

266
00:23:51,009 --> 00:24:00,001
selection pressure. So finally let's turn
to the application of computer vision.

267
00:24:00,001 --> 00:24:06,003
There are many, many examples, and pretty
much these examples, have, at this point

268
00:24:06,003 --> 00:24:12,007
do not require that we construct a full
vision system comparable to a human being,

269
00:24:12,007 --> 00:24:18,005
but we construct some aspect of this
ability and that's enough for us to

270
00:24:18,005 --> 00:24:25,000
construct useful applications. For example
I have some here; Biometrics, identifying

271
00:24:25,000 --> 00:24:30,000
people. So, for example, you want to
control access to some environment, and

272
00:24:30,000 --> 00:24:35,000
you do it by doing face recognition.
Computational photography, this is a

273
00:24:35,000 --> 00:24:40,004
emerging discipline over the last 10-15
years. And essentially, the idea is that

274
00:24:40,004 --> 00:24:45,008
we can attach a computer to the camera,
and then we are no longer stuck with the

275
00:24:45,008 --> 00:24:51,004
passive image captured. We can choose the,
the imaging parameters of the camera to

276
00:24:51,004 --> 00:24:56,007
construct better images. Very simple
example is that nowadays when we take

277
00:24:56,007 --> 00:25:02,005
photographs of when there is some program
which is trying to detect faces.

278
00:25:02,005 --> 00:25:07,007
And.. unable... and what is done with that is
to make sure that the faces

279
00:25:07,007 --> 00:25:12,009
are in focus. Because the expectation is
that when we take images, we really want

280
00:25:12,009 --> 00:25:18,000
the people's faces to be in sharp focus.
Human computer interaction is a leading

281
00:25:18,000 --> 00:25:23,003
application, because, with the help of a
camera, you can, determine the pulse of

282
00:25:23,003 --> 00:25:28,006
the human body. And of course, one of the
leading examples of this is the Microsoft

283
00:25:28,006 --> 00:25:34,001
Kinect system which is used in gaming. We
can create three dimensional virtual

284
00:25:34,001 --> 00:25:39,007
worlds by variations of the techniques
that I talked about earlier for 3D

285
00:25:39,007 --> 00:25:46,001
reconstruction. And, again, this is a very
widespread, Technology now, Content-based

286
00:25:46,001 --> 00:25:51,008
image retrieval. I mean, when you do web
searches, and you want to find examples of

287
00:25:51,008 --> 00:25:57,004
images of certain categories. Sometimes
you can do it based on text, but sometimes

288
00:25:57,004 --> 00:26:02,009
you can also explore image content, and
many, many more. I will not. Continue with

289
00:26:02,009 --> 00:26:07,008
these, this list of examples here. But a
comprehensive list can be found at a

290
00:26:07,008 --> 00:26:13,000
website which is maintained by Derek Law.
And I recommend you to go, check this

291
00:26:13,000 --> 00:26:17,008
website for hundreds of examples of
companies which have computer vision products.

292
00:26:17,008 --> 00:26:24,007
You wouldn't have noticed that
in this lecture I've jumped back and forth

293
00:26:24,007 --> 00:26:30,000
between talking about human vision
and computer vision. And to me, this

294
00:26:30,000 --> 00:26:35,004
is, there is a lot to be learned from
human vision. And Let's try to get

295
00:26:35,004 --> 00:26:40,008
in, a bit into how and why. So, human
vision is studied by many different

296
00:26:40,008 --> 00:26:46,002
scientists. So, perception is a
traditional discipline, typically studied

297
00:26:46,002 --> 00:26:52,002
my psychologists. And here, we consider
the visual system as a black box. We are

298
00:26:52,002 --> 00:26:58,004
trying to figure out what a human would
perceive in an image, and try to construct

299
00:26:58,004 --> 00:27:03,008
a scientific theory for that. Neuroscience
of course gets into the black box, and

300
00:27:03,008 --> 00:27:09,000
it's trying to understand what are the
brain areas associated with, with vision.

301
00:27:09,000 --> 00:27:13,005
And vision of course starts in the retina
but goes on in the brain. What's

302
00:27:13,005 --> 00:27:17,009
interesting is that something like 30 to
50 percent of the brain is devoted to

303
00:27:17,009 --> 00:27:22,008
visual processing. Finally, there is an
understanding of vision at the functional

304
00:27:22,008 --> 00:27:27,008
level, which has to do with how the laws
of optics and the statistics of the world

305
00:27:27,008 --> 00:27:33,004
we live in. Make certain interpretation of
an image much more likely. Now this

306
00:27:33,004 --> 00:27:39,006
version of the problem is equally. Valid
for a human system as well as for a

307
00:27:39,006 --> 00:27:45,002
computer vision system. We are. Both the,
both humans and computer vision systems

308
00:27:45,002 --> 00:27:50,000
are equally constrained by the laws of
optics, and the statistics of the world we

309
00:27:50,000 --> 00:27:54,003
live in. So at the functional level,
there's going to be a very good match.

310
00:27:54,003 --> 00:27:59,003
Perception and neuroscience we do not need
to copy exactly. But often. It has turned

311
00:27:59,003 --> 00:28:04,007
out that both studies of, of human vision
and psychology and neuroscience has been a

312
00:28:04,007 --> 00:28:09,006
source of wonderful inspiration for
computer vision systems. So I'll give you

313
00:28:09,006 --> 00:28:14,008
another example of, the use of, of the
functional level. This is not to do with

314
00:28:14,008 --> 00:28:20,002
vision but flight. When we consider birds
and airplanes, we don't build airplanes by

315
00:28:20,002 --> 00:28:25,003
imitating wings with feathers and bones
and muscles and so on. How are the basic

316
00:28:25,003 --> 00:28:30,009
principles of flight, the basic principles
of lift. Using Bernoulli's principle has

317
00:28:30,009 --> 00:28:37,000
to be the same when we consider birds or.
Happenings. And what we are trying to

318
00:28:37,000 --> 00:28:42,001
uncover are the counterparts of
Bernoulli's principle. That's the science

319
00:28:42,001 --> 00:28:47,006
of vision. And, those are going to be the
same when we, whether we study human

320
00:28:47,006 --> 00:28:58,003
vision or computer vision. I should take
you a little bit through the history of

321
00:28:58,003 --> 00:29:04,000
computer vision. It's a relatively young
discipline. It, it's like any field of

322
00:29:04,000 --> 00:29:09,005
computer science. It's more or less 50
years old. We say, I, I picked this 1963

323
00:29:09,005 --> 00:29:15,005
as a specific year, because this is the,
the, the year when the first PHD thesis in

324
00:29:15,005 --> 00:29:21,007
computer vision, is, was written. And the
beginnings of the field were in artificial

325
00:29:21,007 --> 00:29:27,002
intelligence, imagine processing, and in
pattern recognition. In the 1970s, It

326
00:29:27,002 --> 00:29:32,006
became more of a science and some
traditional areas of applied mathematics

327
00:29:32,006 --> 00:29:38,002
began to be brought in to, to, to bear on
the problem. And pioneers such has Hann,

328
00:29:38,002 --> 00:29:43,005
Condrain, Klong, and Higgins tried to
understand the, the basic principles of

329
00:29:43,005 --> 00:29:48,006
shape from shading or structure from
motion, and so on. The 1980s, we saw a

330
00:29:48,006 --> 00:29:54,000
vision develo ped as a branch of applied
mathematics. And a variety of different

331
00:29:54,000 --> 00:29:59,002
fields of mathematics were, brought to
bear on vision, including geometry,

332
00:29:59,004 --> 00:30:04,003
multiscale analysis, probabilistic
modeling, control theory, optimization,

333
00:30:04,003 --> 00:30:11,002
and what have you. 1990s were very
successful in the geometric aspect

334
00:30:11,002 --> 00:30:17,002
division. So the basic mathematics of
multiple view geometry was worked out

335
00:30:17,002 --> 00:30:23,005
then, and we started to get the fullest
successful algorithms for recovering 3D

336
00:30:23,005 --> 00:30:29,001
structure. This really flowered in the
2000s in terms of practical applications.

337
00:30:29,001 --> 00:30:34,003
But the basics were all, had all been laid
by the 1990s. Statistical learning

338
00:30:34,003 --> 00:30:39,007
approaches. Which had begun even back in
the 1960's, and the rubric of pattern

339
00:30:39,007 --> 00:30:45,005
recognition really came to the fore in the
1990's, and they had their big successes

340
00:30:45,005 --> 00:30:51,001
in the 2000's. So the 2000's we would call
the age of visual recognition,

341
00:30:51,001 --> 00:30:56,004
it's age that saw major progress.
Recall my earlier example, that on this

342
00:30:56,004 --> 00:31:01,001
Cal Tech [inaudible], performance went up
from sixteen percent correct to 65 percent

343
00:31:01,001 --> 00:31:07,005
correct over a period of three years. And
a number of advances have made this

344
00:31:07,005 --> 00:31:12,006
possible. We have. New machine learning
techniques which have been pretty

345
00:31:12,006 --> 00:31:17,009
effective. We have access to large amounts
of data, large collections of images. Some

346
00:31:17,009 --> 00:31:23,002
estimates range up to a trillion images on
the web, Large collection of video. So we

347
00:31:23,002 --> 00:31:27,009
have a large exam, set of data to work
with. We have the computing power to.

348
00:31:27,009 --> 00:31:33,008
Exploit this data and we have techniques
which, have been developed over the last

349
00:31:33,008 --> 00:31:39,006
ten, twenty years which have enabled us to
make progress on this. So in this course

350
00:31:39,006 --> 00:31:45,005
we'll try to go through the basics of all
these. And, I want to welcome you to the

351
00:31:45,005 --> 00:31:50,002
course and just. In conclusion, tell you
what the main topics we'll study are.

352
00:31:50,002 --> 00:31:55,000
We'll begin with the fundamentals of image
formation, because it's important to

353
00:31:55,000 --> 00:31:59,004
understand both the geometry and
radiometry of image formation. After that,

354
00:31:59,004 --> 00:32:04,002
we'll go through basic image processing
operations, linear filtering, convolution

355
00:32:04,002 --> 00:32:09,001
and so on, because this is the basis for
edge detection and finding boundaries, as

356
00:32:09,001 --> 00:32:13,006
well as the measurement of various image
features which would be useful for recognition.

357
00:32:13,006 --> 00:32:18,004
Then I will go through the
three fundamental R's of vision if you will.

358
00:32:18,004 --> 00:32:23,004
Recognition, we'll spend a lot of
time on that. Recognition of objects,

359
00:32:23,004 --> 00:32:28,005
people, activities, Reconstruction. This
is mainly about recovering 3D geometry,

360
00:32:28,005 --> 00:32:33,002
but we'll also study properties such as
reflectants and so on. And then

361
00:32:33,002 --> 00:32:38,001
reorganization, which is, the grouping of
the image into, sets of pixels

362
00:32:38,001 --> 00:32:43,007
corresponding to different objects. I
hope you enjoy the course and I'll see you

363
00:32:43,007 --> 00:32:46,002
at the next lecture. Bye.
